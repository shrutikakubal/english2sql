# New Chatgpt function which can:
# get cost
# pass tokens
# act as classifier fxn

import pandas as pd
from retry import retry
import tiktoken
import inspect
from enum import Enum
from evadb.catalog.catalog_type import NdArrayType
from evadb.functions.abstract.abstract_function import AbstractFunction
from evadb.functions.decorators.decorators import forward, setup
from evadb.functions.decorators.io_descriptors.data_types import PandasDataframe
import os
from evadb.utils.generic_utils import try_to_import_openai

try_to_import_openai()
import openai

# Extended ChatGPT function which can get cost and act as a classifer

_VALID_CHAT_COMPLETION_MODEL = [
    "gpt-4",
    "gpt-4-0314",
    "gpt-4-32k",
    "gpt-4-32k-0314",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-0301",
]


class ExtendedChatGPT(AbstractFunction):
    """
       Arguments:
            model (str) : ID of the OpenAI model to use. Refer to '_VALID_CHAT_COMPLETION_MODEL' for a list of supported models.
            temperature (float) : Sampling temperature to use in the model. Higher value results in a more random output.

        Input Signatures:
            query (str)   : The task / question that the user wants the model to accomplish / respond.
            content (str) : Any relevant context that the model can use to complete its tasks and generate the response.
            prompt (str)  : An optional prompt that can be passed to the model. It can contain instructions to the model,
                            or a set of examples to help the model generate a better response.
                            If not provided, the system prompt defaults to that of an helpful assistant that accomplishes user tasks.
            labels (Enum) : An enum of possible labels for the classifier. The enum values can be a dictionary of hints for the classifier.
            return_cost (bool) : If true, returns the cost of the query in terms of tokens used. If false, returns only the response.

        Output Signatures:
            response (str) : Contains the response generated by the model based on user input. Any errors encountered
                             will also be passed in the response.
            cost (str) : Contains the cost of the query in terms of tokens used. Returned with the response if return_cost is set to true.
    """
    @property
    def name(self) -> str:
        return "ExtendedChatGPT"

    @setup(cacheable=True, function_type="chat-completion", batchable=True)
    def setup(
        self,
        model="gpt-3.5-turbo",
        temperature: float = 0,
        openai_api_key="",
    ) -> None:
        assert model in _VALID_CHAT_COMPLETION_MODEL, f"Unsupported ChatGPT {model}"
        self.model = model
        self.temperature = temperature
        self.openai_api_key = openai_api_key

    @forward(
        input_signatures=[
            PandasDataframe(
                columns=["query", "content", "prompt",
                         "labels", "return_cost"],
                column_types=[
                    NdArrayType.STR,
                    NdArrayType.STR,
                    NdArrayType.STR,
                    NdArrayType.ANYTYPE,
                    NdArrayType.BOOL,
                ],
                column_shapes=[(1,), (1,), (None,), None, None],
            )
        ],
        output_signatures=[
            PandasDataframe(
                columns=["response", "cost"],
                column_types=[
                    NdArrayType.STR,
                    NdArrayType.STR,
                ],
                column_shapes=[(1,), (1,)],
            )
        ],
    )
    def forward(self, text_df):

        try_to_import_openai()
        # shruti
        # from openai import OpenAI
        import openai

        api_key = self.openai_api_key
        if len(self.openai_api_key) == 0:
            api_key = os.environ.get("OPENAI_API_KEY", "")
        assert (
            len(api_key) != 0
        ), "Please set your OpenAI API key using SET OPENAI_API_KEY = 'sk-' or environment variable (OPENAI_API_KEY)"

        # shruti
        # client = OpenAI(api_key=api_key)
        openai.api_key = api_key

        @retry(tries=6, delay=20)
        def completion_with_backoff(**kwargs):
            # shruti
            # return client.chat.completions.create(**kwargs)
            return openai.ChatCompletion.create(**kwargs)
        
        # Create a logit bias dictionary for the classifier - convert words to tokens and set bias to max
        def CreateLogitBiasDict(enum_list: Enum, _enumerate: bool):
          values = [val.name for val in enum_list]
          encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
          logit_bias = {str(encoded): 100
                      for val, token in enumerate(values)
                      for encoded in encoding.encode(str(val) if _enumerate else str(token))
                      }
          return logit_bias

      # Create a prompt for the classifier using the labels and hints
        def createPrompt(classes: Enum, description: str):
          hint_str = ""
          for val in classes:
              if not isinstance(val.value, dict):
                  break
              hint_str += str(val.value)+"\n"

          # pass the classes as a list of enums to the prompt
          prompt = f"You are an expert classifier. Classify the content into one of the following options: {[val.name for val in classes]} based on the request. Each option represents the {description}"
          if hint_str != "":
            prompt += f" Use the following hints:\n{hint_str}"
          return inspect.cleandoc(prompt)

        queries = text_df[text_df.columns[0]]
        content = text_df[text_df.columns[0]]
        if len(text_df.columns) > 1:
            queries = text_df.iloc[:, 0]
            content = text_df.iloc[:, 1]

        prompt = None
        if len(text_df.columns) > 2:
            prompt = text_df.iloc[0, 2]
            labels = text_df.iloc[0, 3]
            return_cost = text_df.iloc[0, 4]

    # formulate prompt
        prompt = createPrompt(
            labels, "possible answer to the user's question. Use the schema of the table in the context to answer.")

        # formulate logit bias
        log_bias = CreateLogitBiasDict(labels, False)

        # act as classifier? - If true, restrict output to one token
        classifier = True
        if (classifier):
            max_tokens = 1
        else:
            max_tokens = 0

        # openai api currently supports answers to a single prompt only
        # so this function is designed for that
        results = []
        tokens = []  # to count usage

        for query, content in zip(queries, content):
            params = {
                "model": "gpt-3.5-turbo",
                "temperature": 0,
                "messages": [],
            }
            if (classifier):
                params = params | {
                    "max_tokens": max_tokens, "logit_bias": log_bias}

            def_sys_prompt_message = {
                "role": "system",
                "content": prompt
                if prompt is not None
                else "You are a helpful assistant that accomplishes user tasks.",
            }

            params["messages"].append(def_sys_prompt_message)
            params["messages"].extend(
                [
                    {
                        "role": "user",
                        "content": f"Context : {content}",
                    },
                    {
                        "role": "user",
                        "content": f"Answer the following question: {query}",
                    },
                ],
            )

            response = completion_with_backoff(**params)
            answer = response.choices[0].message.content
            results.append(answer)
            tokens.append([response.usage.prompt_tokens,
                          response.usage.completion_tokens, response.usage.total_tokens])

        df1 = pd.DataFrame({"response": results})
        df2 = pd.DataFrame(tokens, columns=["prompt", "completion", "total"])
        df3 = pd.concat([df1, df2], ignore_index=True, axis=1)
        df3.columns = ['response', "prompt", "completion", "total"]
        if return_cost:
            return df3
        else:
            return df1
